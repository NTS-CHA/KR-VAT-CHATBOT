from fastapi import FastAPI, Request
from fastapi.responses import FileResponse
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel
from openai import OpenAI
from typing import Literal
import os, time, json, requests, csv
from dotenv import load_dotenv
from pathlib import Path
import re
import tiktoken
import inspect
from functools import lru_cache
from datetime import datetime
import unicodedata
import pandas as pd
import matplotlib.pyplot as plt


# log_file_path = "logs/gpt_calls.csv"
# if os.path.exists(log_file_path):
#     df = pd.read_csv(log_file_path)
#     if "purpose" in df.columns and not df.empty:
#         df.groupby("purpose")["cost"].sum().plot(kind="bar")


purpose_map = {
    "ask_with_law_context": "ì „ì²´ ì‘ë‹µ",
    "summarize_reference_tags": "ì¡°ë¬¸ ìš”ì•½",
    "extract_law_reference_mapping": "ì˜ˆì‹œ ì¶”ì¶œ",
    "smart_translate_law_tag": "ë²ˆì—­",
    "self_rate_answer": "ì‹ ë¢°ë„ í‰ê°€",
    "extract_summary": "ìš”ì•½ ì¶”ì¶œ",
    "auto_infer_references": "ì¡°ë¬¸ ì¶”ë¡ ",
    "translate_law_tag_gpt_cached": "ë²ˆì—­ ìºì‹œ",
    "extract_all_law_parts_gpt": "ì¡°ë¬¸ ë¶„ë¦¬"
}

GPT_MODEL_MAIN = "gpt-4"
GPT_MODEL_LIGHT = "gpt-3.5-turbo"

load_dotenv()
app = FastAPI()
app.mount("/static", StaticFiles(directory="static"), name="static")
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
if not os.getenv("OPENAI_API_KEY"):
    raise RuntimeError("âŒ OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

_translation_cache = {}
log_file = "logs/gpt_calls.csv"
os.makedirs("logs", exist_ok=True)


if not os.path.exists(log_file):
    with open(log_file, "w", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        writer.writerow(["timestamp", "function", "model", "prompt_tokens", "output_tokens", "total_tokens", "cost", "duration", "purpose"])

def normalize_ref(ref: str) -> str:
    return re.sub(r"\s+", "", ref.strip())

def clean_input(text: str) -> str:
    # NFC ì •ê·œí™” + surrogate ì œê±°
    text = unicodedata.normalize("NFC", text)
    return "".join(c for c in text if not unicodedata.category(c).startswith("Cs"))

def sanitize_messages(messages):
    return [
        {**msg, "content": clean_input(msg.get("content", ""))}
        for msg in messages
    ]

def log_gpt_call(model, caller, prompt_tokens, output_tokens, total_tokens, cost, duration, purpose):
    try:
        with open(log_file, "a", newline="", encoding="utf-8") as f:
            writer = csv.writer(f)
            writer.writerow([
                datetime.now().isoformat(), caller, model,
                prompt_tokens, output_tokens, total_tokens,
                f"{cost:.6f}", f"{duration:.3f}", purpose
            ])
    except UnicodeEncodeError as e:
        print(f"âš ï¸ ë¡œê·¸ ì €ì¥ ìƒëµ: {e}")

def gpt_call(model, messages, temperature=0.2, timeout=30):
    start = time.time()  # â± í˜¸ì¶œ ì‹œê°„ ì¸¡ì • ì‹œì‘
    caller = inspect.stack()[1].function
    purpose = purpose_map.get(caller, caller)
    messages = sanitize_messages(messages)

    try:
        encoding = tiktoken.encoding_for_model(model)
        prompt_tokens = sum(len(encoding.encode(msg.get("content", ""))) for msg in messages)

        completion = client.chat.completions.create(
            model=model,
            messages=messages,
            temperature=temperature,
            timeout=timeout
        )

        output_text = completion.choices[0].message.content or ""
        output_text = unicodedata.normalize("NFC", output_text)
        output_text = ''.join(c for c in output_text if not 0xD800 <= ord(c) <= 0xDFFF)

        try:
            output_tokens = len(encoding.encode(output_text))
        except Exception as token_err:
            print(f"âš ï¸ í† í° ì¸ì½”ë”© ì‹¤íŒ¨, ë¬´ì‹œë¨: {token_err}")
            output_tokens = 0

        total_tokens = prompt_tokens + output_tokens
        cost = (
            prompt_tokens * 0.03 / 1000 + output_tokens * 0.06 / 1000 if "gpt-4" in model
            else prompt_tokens * 0.0015 / 1000 + output_tokens * 0.002 / 1000 if "gpt-3.5" in model
            else 0.0
        )
        duration = round(time.time() - start, 3)

        try:
            print(f"\U0001f9e0 GPT Call | {caller} | model={model} | tokens={prompt_tokens}+{output_tokens}={total_tokens} | ğŸ’° ${cost:.4f}")
        except UnicodeEncodeError:
            print(f"âœ… GPT í˜¸ì¶œ ì™„ë£Œ (log ì¶œë ¥ ìƒëµë¨ â€” surrogate í¬í•¨ ê°€ëŠ¥ì„±)")

        log_gpt_call(model, caller, prompt_tokens, output_tokens, total_tokens, cost, duration, purpose)
        return completion

    except Exception as e:
        duration = round(time.time() - start, 3)
        print(f"âŒ GPT í˜¸ì¶œ ì‹¤íŒ¨ in {caller} | model={model} | error={e}")
        log_gpt_call(model, caller, 0, 0, 0, 0.0, duration, purpose)
        raise

def safe_gpt_call(*args, retries=2, **kwargs):
    for i in range(retries):
        try:
            return gpt_call(*args, **kwargs)
        except Exception as e:
            print(f"âŒ GPT ì‹¤íŒ¨ {i+1}/{retries}: {e}")
            time.sleep(1.5 ** i)
    return None

@lru_cache(maxsize=512)
def translate_law_tag_gpt_cached(tag: str) -> str:
    try:
        completion = gpt_call(
            model=GPT_MODEL_LIGHT,
            messages=[
                {
                    "role": "system",
                    "content": "Translate this Korean legal article reference to an English version suitable for legal citation. Format like: VAT Act Article 53-2 (1) (i)"
                },
                {"role": "user", "content": tag}
            ],
            temperature=0.1,
            timeout=30
        )
        return completion.choices[0].message.content.strip()
    except Exception as e:
        return "(ë²ˆì—­ ì‹¤íŒ¨)"

def translate_law_tag_gpt(tags: list[str]) -> dict[str, str]:
    return {tag: translate_law_tag_gpt_cached(tag) for tag in tags}

def translate_law_tag_regex(kor: str) -> str:
    kor = kor.strip().replace(" ", "")
    kor = kor.replace("ë¶€ê°€ê°€ì¹˜ì„¸ë²•", "VAT Act").replace("ì‹œí–‰ë ¹", "Enforcement Decree").replace("ì‹œí–‰ê·œì¹™", "Enforcement Rule")
    match = re.match(r"(.*?)(ì œ\d+ì¡°(?:ì˜?\d+)?)(ì œ\d+í•­)?(ì œ\d+í˜¸)?", kor)
    if not match:
        return kor
    base, article, clause, item = match.groups()
    article = article.replace("ì œ", "").replace("ì¡°ì˜", "-").replace("ì¡°", "")
    clause = f" ({clause.replace('ì œ', '').replace('í•­', '')})" if clause else ""
    item = f" ({item.replace('ì œ', '').replace('í˜¸', '')})" if item else ""
    return f"{base} Article {article}{clause}{item}".strip()


@lru_cache(maxsize=1024)
def smart_translate_law_tag(tag: str) -> str:
    norm = normalize_ref(tag)
    # âœ… íŒë¡€ë²ˆí˜¸ì¼ ê²½ìš°: Case No. ìœ ì§€
    if re.match(r"^\d{4}(ëˆ„|ë‘|ê³ |ì´ˆ|í˜•|ë§ˆ|ì¬)\d+$", tag) or re.match(r"^\d{4}(Nu|Du|Go|Cho|Hyeong|Ma|Jae)\d+$", tag):
        return f"Case No. {tag}"
    
    if norm in _translation_cache:
        return _translation_cache[norm]

    # âœ… fallbackìš© ì •ê·œì‹ ë²ˆì—­ ì¤€ë¹„
    fallback = translate_law_tag_regex(tag)

    try:
        completion = gpt_call(
            model=GPT_MODEL_LIGHT,
            messages=[
                {
                    "role": "system",
                    "content": "Translate this Korean legal article reference to an English version suitable for legal citation. Format like: VAT Act Article 53-2 (1) (i)"
                },
                {"role": "user", "content": tag}
            ],
            temperature=0.1,
            timeout=30
        )
        result = completion.choices[0].message.content.strip()

        # âœ… ê³µë°± í¬ë§· ë³´ì •
        result = re.sub(
            r"\b(VAT|Enforcement)(Act|Decree|Rule)(Article)\s*([0-9]+(?:\([^)]+\))?)\b",
            r"\1 \2 \3 \4",
            result
        )
        result = re.sub(r"\s+", " ", result).strip()
        _translation_cache[norm] = result
        return result

    except Exception as e:
        print(f"âš ï¸ GPT ì¡°ë¬¸ ë²ˆì—­ ì‹¤íŒ¨: {e}")
        _translation_cache[norm] = fallback
        return fallback

class Query(BaseModel):
    question: str
    lang: Literal["ko", "en"] = "ko"
    law_id: str = "ë¶€ê°€ê°€ì¹˜ì„¸ë²•"
    model: str = "gpt-4"  

def parse_law_text_to_dict(law_text: str) -> dict:
    lines = law_text.splitlines()
    current_key = ""
    mapping = {}
    for line in lines:
        if re.match(r"\[.*ë²•.*\]", line.strip()):
            current_key = line.strip("[]").strip()
            mapping[current_key] = ""
        elif current_key:
            mapping[current_key] += line + "\n"
    return mapping

def self_rate_answer(question: str, answer: str) -> int:
    try:
        completion = gpt_call(
            model=GPT_MODEL_LIGHT,
            messages=[
                {"role": "system", "content": "You are a legal accuracy evaluator. Rate how confident you are in the accuracy of this answer (0-100). Respond only like: Confidence: 87"},
                {"role": "user", "content": f"Question: {question}\nAnswer: {answer}"}
            ],
            temperature=0.2,
            timeout=30
        )
        line = completion.choices[0].message.content.strip()
        if "Confidence" in line:
            return int("".join(filter(str.isdigit, line)))
    except Exception:
        pass
    return 0

def extract_law_references_and_omission(answer: str) -> tuple[list[str], dict]:
    bracketed = re.findall(r"\[(.*?)\]", answer)
    # loose = re.findall(r"ë¶€ê°€ê°€ì¹˜ì„¸ë²•\s?ì œ\d+ì¡°", answer)
    # law_tags = re.findall(r"ë¶€ê°€ê°€ì¹˜ì„¸ë²•\s?ì œ(?:\d+ì¡°(?:ì˜?\d*)?(?:\s?ì œ\d+í•­)?(?:\s?ì œ\d+í˜¸)?)", answer)
    law_tags = re.findall(r"ë¶€ê°€ê°€ì¹˜ì„¸ë²•\s?ì œ[\dì¡°ì˜í•­í˜¸\s]+", answer)


    case_tags = re.findall(r"\d{4}ë‘\d+", answer)
    refs = list(set(r.strip() for r in bracketed + law_tags + case_tags))
    

    total = len(set(law_tags))
    present = sum(1 for ref in set(law_tags) if any(ref in b for b in bracketed))
    missing = total - present
    stats = {
        "total_refs": total,
        "bracketed": present,
        "omitted": missing,
        "omission_rate": round(missing / total * 100, 1) if total else 0
    }
    return refs, stats

def extract_law_reference_mapping(answer: str, refs: list[str], lang="ko", tag_law_map: dict[str, str] = None) -> dict[str, str]:
    mapping = {}
    refs = deduplicate_refs(refs)

    sys_msg = (
        "Extract 1 sentence from the answer that clearly includes this law reference. Output in Korean."
        if lang == "ko"
        else
        "Extract 1 sentence from the answer that clearly includes this law reference. Output in English."
    )
    for ref in refs:
        law_text = tag_law_map.get(ref, "") if tag_law_map else ""
        law_text = clean_input(law_text)
        try:
            completion = gpt_call(
                model=GPT_MODEL_MAIN,
                messages=[
                    {"role": "system", "content": sys_msg},
                    {"role": "user", "content": f"Answer: {answer}\nReference: {ref}\nLaw: {law_text}"}
                ],
                temperature=0.1,
                timeout=30
            )
            mapping[ref] = completion.choices[0].message.content.strip()
        except Exception:
            mapping[ref] = "(ë§¤í•‘ ì‹¤íŒ¨)"
    return mapping

def is_korean_case_tag(tag: str) -> bool:
    return bool(re.match(r"^\d{4}(ëˆ„|ë‘|ê³ |ì´ˆ|í˜•|ë§ˆ|ì¬)\d+$", tag))

def is_english_case_tag(tag: str) -> bool:
    return bool(re.match(r"^\d{4}(Nu|Du|Go|Cho|Hyeong|Ma|Jae)\d+$", tag))

def is_case_tag(tag: str) -> bool:
    return is_korean_case_tag(tag) or is_english_case_tag(tag)


def summarize_reference_tags(refs: list[str], lang="ko", tag_law_map: dict[str, str] = None) -> dict:
    summaries = {}
    total_time = 0
    count = 0
    refs = deduplicate_refs(refs)

    for tag in refs:
        start = time.time()

        if is_case_tag(tag):
            if lang == "en":
                summaries[tag] = f"Case No. {tag} refers to a Korean Supreme Court decision from the year {tag[:4]}."
            else:
                summaries[tag] = f"[{tag}]ëŠ” ëŒ€í•œë¯¼êµ­ {tag[:4]}ë…„ì˜ íŒë¡€ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤."
            continue

        law_text = tag_law_map.get(tag, "") if tag_law_map else ""
        sys_msg = (
            "Summarize what this Korean legal tag refers to in 1 sentence. Output in Korean."
            if lang == "ko"
            else
            "Summarize what this Korean legal tag refers to in 1 sentence. Output in English."
        )
        try:
            completion = gpt_call(
                model=GPT_MODEL_MAIN,
                messages=[
                    {"role": "system", "content": sys_msg},
                    {"role": "user", "content": f"[{tag}]\n{law_text}"}
                ],
                temperature=0.2,
                timeout=15
            )
            summary = completion.choices[0].message.content.strip()
            elapsed = time.time() - start
            total_time += elapsed
            count += 1

            summaries[tag] = summary if elapsed <= 14 else "(ìš”ì•½ ìƒëµ - ì‹œê°„ ì´ˆê³¼)"
        except:
            summaries[tag] = "ìš”ì•½ ì‹¤íŒ¨"

    if count:
        print(f"ğŸ“Š í‰ê·  ìš”ì•½ ì‹œê°„: {total_time/count:.2f}ì´ˆ ({count}ê°œ íƒœê·¸)")
    return summaries


def auto_infer_references(question: str, answer: str) -> list[str]:
    try:
        completion = gpt_call(
            model=GPT_MODEL_MAIN,
            messages=[
                {"role": "system", "content": "ì‚¬ìš©ì ì§ˆë¬¸ê³¼ GPTì˜ ì‘ë‹µì„ ë°”íƒ•ìœ¼ë¡œ ê´€ë ¨ëœ í•œêµ­ ì„¸ë²• ì¡°ë¬¸ ì´ë¦„ì„ ìµœëŒ€ 3ê°œê¹Œì§€ ì¶”ì¶œí•´ì¤˜. ë°˜ë“œì‹œ [ì¡°ë¬¸ëª…] í˜•íƒœë¡œ ëŒ€ê´„í˜¸ë¡œ ê°ì‹¸ì„œ ëª©ë¡ìœ¼ë¡œ ì¶œë ¥í•´. ì˜ˆ: [ë¶€ê°€ê°€ì¹˜ì„¸ë²• ì œ25ì¡°]"},
                {"role": "user", "content": f"ì§ˆë¬¸: {question}\nì‘ë‹µ: {answer}"}
            ],
            temperature=0.2,
            timeout=15
        )
        return re.findall(r"\\[(.*?)\\]", completion.choices[0].message.content)
    except Exception as e:
        print(f"âš ï¸ íƒœê·¸ ìë™ì¶”ë¡  ì‹¤íŒ¨: {e}")
        return []
 
def ask_with_law_context(question: str, law_text: str, precedents: str = "", lang: str = "ko", original_question: str = "") -> tuple[str, list[str], dict, dict]:
    system_prompt = (
        "All outputs must be in English.\n"
        "You are a Korean tax attorney specialized in Korean VAT law.\n"
        "Answer all questions strictly based on Korean VAT law and precedents.\n"
        "Always cite Korean VAT articles using tag format like [VAT Act Article 29 (1) 2].\n"
        "Avoid repeating the same article multiple times.\n"
        "Clearly explain why each law applies, with structured logic.\n"
        "If your conclusion and reasoning conflict, revise your answer to resolve contradictions.\n"
        "If a policy or specific administrative rule overrides a general VAT article, the policy takes precedence. Explicitly explain such overrides when applicable.\n"
        "Use precedents like [Case No. 2017Du34481] when relevant.\n"
        "Your output must follow this structure:\n"
        "1. Conclusion (short)\n"
        "2. Reasoning (structured)\n"
        "3. References (tagged articles + cases)"
        if lang == "en" else
        "ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ ë¶€ê°€ê°€ì¹˜ì„¸ë²• ì „ë¬¸ê°€ì…ë‹ˆë‹¤.\n"
        "ë‹µë³€ì—ëŠ” ë°˜ë“œì‹œ ê´€ë ¨ ì¡°ë¬¸ì„ íƒœê·¸ í˜•ì‹ìœ¼ë¡œ ì œì‹œí•˜ì‹­ì‹œì˜¤. ì˜ˆ: [ë¶€ê°€ê°€ì¹˜ì„¸ë²• ì œ26ì¡° ì œ1í•­ ì œ2í˜¸]\n"
        "ê°™ì€ ì¡°ë¬¸ì€ ë°˜ë³µí•˜ì§€ ë§ˆì‹­ì‹œì˜¤.\n"
        "ì¡°ë¬¸ì´ ì ìš©ë˜ëŠ” ì´ìœ ë¥¼ ëª…í™•íˆ ë…¼ë¦¬ì ìœ¼ë¡œ ì„¤ëª…í•˜ì‹­ì‹œì˜¤.\n"
        "ê²°ë¡ ê³¼ ì´ìœ ê°€ ì¶©ëŒí•˜ê±°ë‚˜ ëª¨ìˆœë˜ì§€ ì•Šë„ë¡ í•˜ì‹­ì‹œì˜¤.\n"
        "íŠ¹ì • í–‰ì • ì§€ì¹¨ì´ë‚˜ ì •ì±…ì´ ì¼ë°˜ì ì¸ ë¶€ê°€ê°€ì¹˜ì„¸ë²• ì¡°ë¬¸ë³´ë‹¤ ìš°ì„ í•˜ëŠ” ê²½ìš°, ê·¸ ì •ì±…ì„ ë¨¼ì € ì ìš©í•˜ê³  ê·¸ ì´ìœ ë¥¼ ëª…í™•íˆ ì„¤ëª…í•˜ì‹­ì‹œì˜¤.\n"
        "íŒë¡€ê°€ ìˆìœ¼ë©´ ë°˜ë“œì‹œ ì–¸ê¸‰í•˜ê³  ê°„ë‹¨íˆ ìš”ì•½í•˜ì‹­ì‹œì˜¤.\n"
        "ë‹µë³€ êµ¬ì¡°ëŠ”:\n"
        "1. ê²°ë¡ \n"
        "2. ì´ìœ \n"
        "3. ì¡°ë¬¸ ë° íŒë¡€"
    )

    law_dict = parse_law_text_to_dict(law_text)

    if lang == "en" and original_question:
        content = f"""Relevant Law:\n{law_text}
            Precedents:\n{precedents}
            Original User Question (in English):
            {original_question}
            Translated to Korean for legal analysis:
            {question}
            """
    else:
        content = f"""Relevant Law:\n{law_text}
            Precedents:\n{precedents}
            Question:\n{question}
            """

    try:
        response = safe_gpt_call(
            model=GPT_MODEL_MAIN,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": content}
            ],
            temperature=0.3
        )
        if not response:
            raise RuntimeError("GPT ì‘ë‹µ ì‹¤íŒ¨")

        answer = response.choices[0].message.content.strip()
        answer = clean_input(answer)
        refs, omission_stats = extract_law_references_and_omission(answer)

        if not refs:
            refs = auto_infer_references(question, answer)

        refs = deduplicate_refs(refs)
        tag_law_map = {tag: law_dict.get(tag, "") for tag in refs}

        # í†µí•© ìš”ì•½ + ì˜ˆì‹œ
        sys_msg = (
            "For each tag below, summarize it in 1 sentence and give 1 example usage from the answer. Output JSON:\n"
            "{ 'ì¡°ë¬¸ëª…': { 'summary': '...', 'example': '...' } }\n"
            "Use Korean." if lang == "ko" else
            "For each tag below, summarize it in 1 sentence and give 1 example usage from the answer. Output JSON:\n"
            "{ 'Tag Name': { 'summary': '...', 'example': '...' } }"
        )

        prompt = f"Answer:\n{answer}\n\nTags:\n" + "\n".join([f"- {ref}" for ref in refs])

        completion = safe_gpt_call(
            model=GPT_MODEL_MAIN,
            messages=[
                {"role": "system", "content": sys_msg},
                {"role": "user", "content": prompt}
            ],
            temperature=0.2,
            timeout=30
        )
        if not completion:
            raise RuntimeError("GPT íƒœê·¸ ìš”ì•½ ì‹¤íŒ¨")

        combined_json = completion.choices[0].message.content.strip()

        try:
            parsed = json.loads(combined_json)
            summaries = {k: fix_tag_spacing(v["summary"]) for k, v in parsed.items()}
            mappings = {k: fix_tag_spacing(v["example"]) for k, v in parsed.items()}
        except Exception as e:
            print(f"âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
            summaries = summarize_reference_tags(refs, lang, tag_law_map)
            mappings = extract_law_reference_mapping(answer, refs, lang, tag_law_map)

        return answer, refs, summaries, mappings
    except Exception as e:
        print(f"âŒ GPT ì˜¤ë¥˜: {e}")
        return {
            "answer": f"âŒ GPT ì˜¤ë¥˜: {str(e)}",
            "references": [],
            "summaries": {},
            "mappings": {},
            "confidence": 0
        }

    
def extract_summary(answer: str, lang: str = "ko") -> str:
    try:
        completion = gpt_call(
            model=GPT_MODEL_LIGHT,
            messages=[
                {"role": "system", "content": (
                    "Summarize the key legal conclusion of this answer in 1 sentence. Output in Korean." if lang == "ko"
                    else "Summarize the key legal conclusion of this answer in 1 sentence. Output in English."
                )},
                {"role": "user", "content": answer}
            ],
            temperature=0.2,
            timeout=10
        )
        return completion.choices[0].message.content.strip()
    except:
        return ""


@lru_cache(maxsize=64)
def fetch_combined_laws_cached(law_ids_tuple: tuple, output: str = "XML", lang: str = "ko") -> str:
    return fetch_combined_laws(list(law_ids_tuple), output, lang)


def fetch_combined_laws(law_ids: list[str], output: str = "XML", lang: str = "ko") -> str:
    oc_key = os.getenv("LAW_OC_ID")
    if not oc_key:
        raise RuntimeError("âŒ í™˜ê²½ë³€ìˆ˜ LAW_OC_IDê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

    combined = ""
    for law_id in law_ids:
        params = {
            "OC": oc_key,
            "target": "law",
            "type": output,
            "ID": law_id
        }
        try:
            r = requests.get("https://www.law.go.kr/DRF/lawService.do", params=params, timeout=30)
            if r.status_code == 200:
                combined += f"\n\n[{law_id}]\n" + r.text
        except:
            continue

    if combined:
        return combined
    else:
        return (
            "âš ï¸ ë²•ë ¹ì„ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\n"
            "ì¸í„°ë„· ì—°ê²° ìƒíƒœë¥¼ í™•ì¸í•˜ê±°ë‚˜, ë²•ì œì²˜ APIì—ì„œ í•´ë‹¹ ë²•ë ¹ì´ ì œê³µë˜ëŠ”ì§€ í™•ì¸í•´ ì£¼ì„¸ìš”.\n"
            "ë¬¸ì œê°€ ì§€ì†ë˜ë©´ ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”."
            if lang == "ko" else
            "âš ï¸ Failed to fetch law text.\n"
            "Please check your internet connection or verify if the requested law exists in the Korean Ministry of Government Legislation API.\n"
            "If the problem persists, contact the administrator."
        )


def search_prec_id(keyword: str, max_count: int = 1) -> list[str]:
    """íŒë¡€ë²ˆí˜¸ ë˜ëŠ” í‚¤ì›Œë“œë¡œ íŒë¡€ì¼ë ¨ë²ˆí˜¸(ID) ëª©ë¡ ê²€ìƒ‰"""
    url = "https://www.law.go.kr/DRF/lawSearch.do"
    params = {
        "OC": os.getenv("LAW_OC_ID", "elapse64"),
        "target": "prec",
        "query": keyword,
        "display": max_count,
        "type": "XML"
    }
    try:
        r = requests.get(url, params=params, timeout=20)
        r.raise_for_status()
        from xml.etree import ElementTree as ET
        root = ET.fromstring(r.text)
        return [el.findtext("íŒë¡€ì¼ë ¨ë²ˆí˜¸") for el in root.findall(".//í•­ëª©") if el.findtext("íŒë¡€ì¼ë ¨ë²ˆí˜¸")]
    except Exception as e:
        print(f"âš ï¸ íŒë¡€ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        return []

def fetch_precedents_full(keyword: str, max_count: int = 2) -> str:
    """GPTìš© íŒë¡€ í…ìŠ¤íŠ¸ ìƒì„± â€“ ì „ë¬¸ ìš°ì„ , ì‹¤íŒ¨ ì‹œ ìš”ì•½"""
    def search_prec_id(keyword: str, max_count: int = 1) -> list[str]:
        url = "https://www.law.go.kr/DRF/lawSearch.do"
        params = {
            "OC": os.getenv("LAW_OC_ID", "elapse64"),
            "target": "prec",
            "query": keyword,
            "display": max_count,
            "type": "XML"
        }
        try:
            r = requests.get(url, params=params, timeout=20)
            r.raise_for_status()
            from xml.etree import ElementTree as ET
            root = ET.fromstring(r.text)
            return [el.findtext("íŒë¡€ì¼ë ¨ë²ˆí˜¸") for el in root.findall(".//í•­ëª©") if el.findtext("íŒë¡€ì¼ë ¨ë²ˆí˜¸")]
        except Exception as e:
            print(f"âš ï¸ íŒë¡€ ID ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []

    def fetch_prec_text(prec_id: str) -> str:
        url = "https://www.law.go.kr/DRF/precService.do"
        params = {
            "OC": os.getenv("LAW_OC_ID", "elapse64"),
            "target": "prec",
            "ID": prec_id,
            "type": "XML"
        }
        try:
            r = requests.get(url, params=params, timeout=20)
            r.raise_for_status()
            from xml.etree import ElementTree as ET
            root = ET.fromstring(r.text)
            parts = [
                root.findtext("íŒë¡€ëª…", ""),
                root.findtext("ì‚¬ê±´ë²ˆí˜¸", ""),
                root.findtext("íŒì‹œì‚¬í•­", ""),
                root.findtext("íŒê²°ìš”ì§€", ""),
                root.findtext("ë³¸ë¬¸", "")
            ]
            return "\n\n".join([p.strip() for p in parts if p.strip()])
        except Exception as e:
            print(f"âš ï¸ íŒë¡€ ì „ë¬¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            return ""

    # 1. ID ê²€ìƒ‰
    ids = search_prec_id(keyword, max_count)

    # 2. íŒë¡€ ì „ë¬¸ ê°€ì ¸ì˜¤ê¸°
    full_texts = [fetch_prec_text(pid) for pid in ids]
    full_texts = [txt for txt in full_texts if txt]

    if full_texts:
        return "\n\n---\n\n".join(full_texts)

    # 3. ì‹¤íŒ¨ ì‹œ fallback â†’ ìš”ì•½ í…ìŠ¤íŠ¸
    try:
        search_url = "https://www.law.go.kr/DRF/lawSearch.do"
        params = {
            "OC": os.getenv("LAW_OC_ID", "elapse64"),
            "target": "prec",
            "query": keyword,
            "display": max_count,
            "type": "XML"
        }
        r = requests.get(search_url, params=params, timeout=30)
        r.raise_for_status()
        from xml.etree import ElementTree as ET
        root = ET.fromstring(r.text)
        entries = []
        for case in root.findall(".//í•­ëª©"):
            num = case.findtext("ì‚¬ê±´ë²ˆí˜¸", "")
            title = case.findtext("íŒë¡€ëª…", "")
            gist = case.findtext("íŒê²°ìš”ì§€", "")
            entries.append(f"[{num}] {title}\nìš”ì§€: {gist}")
        return "\n\n".join(entries) if entries else "(âš ï¸ íŒë¡€ ì—†ìŒ)"
    except Exception as e:
        return f"(âš ï¸ íŒë¡€ ê²€ìƒ‰ ì‹¤íŒ¨: {e})"

def extract_all_law_parts_gpt(text: str) -> list[str]:
    try:
        completion = gpt_call(
            model=GPT_MODEL_LIGHT,
            messages=[
                {"role": "system", "content": "Extract every Korean legal reference from the text. Return only a list. Example: ['ë¶€ê°€ê°€ì¹˜ì„¸ë²• ì œ53ì¡°ì˜2', 'ì œ1í•­', 'ì œ1í˜¸']"},
                {"role": "user", "content": text}
            ],
            temperature=0.1,
            timeout=30
        )
        content = completion.choices[0].message.content
        extracted = re.findall(r"'([^']+)'", content)
        return extracted
    except Exception as e:
        print("GPT ì¡°ë¬¸ë¶„ë¦¬ ì‹¤íŒ¨:", e)
        return []

def fix_tag_spacing(text: str) -> str:
    # [VATActArticle8(1)1] â†’ [VAT Act Article 8 (1) (1)]
    text = re.sub(r"(VAT|Enforcement)(Act|Decree|Rule)(Article)", r"\1 \2 \3", text)
    text = re.sub(r"(?<=Article)\s*(\d+)", r" \1", text)
    text = re.sub(r"\)\(", ") (", text)
    return re.sub(r"\s+", " ", text).strip()


def format_english_law_tag(tag: str) -> str:
    # VATActArticle60(1)(i) â†’ VAT Act Article 60 (1) (i)
    tag = re.sub(r"(VAT|Enforcement)(Act|Decree|Rule)(Article)", r"\1 \2 \3", tag)
    tag = re.sub(r"(?<=Article)\s*(\d+)", r" \1", tag)
    tag = re.sub(r"\)\(", ") (", tag)  # (1)(i) â†’ (1) (i)
    return re.sub(r"\s+", " ", tag).strip()

# ì¤‘ë³µëœ ì¡°ë¬¸ íƒœê·¸ ì œê±° (normalize ê¸°ì¤€)
def deduplicate_refs(refs: list[str]) -> list[str]:
    normalized = {}
    for r in refs:
        key = normalize_ref(r)
        if key not in normalized:
            normalized[key] = r
    return list(normalized.values())

@app.post("/ask")
async def ask(query: Query, request: Request):
    translated_question = query.question
    model_used = query.model or GPT_MODEL_MAIN
    if query.lang == "en":
        try:
            completion = gpt_call(
                model=model_used,
                messages=[
                    {
                        "role": "system",
                        "content": "Translate the following English tax question into Korean. Use legal tone and terminology. Only return the translation."
                    },
                    {
                        "role": "user",
                        "content": query.question
                    }
                ],
                temperature=0.1,
                timeout=30
            )
            translated_question = completion.choices[0].message.content.strip()
            translated_question = clean_input(translated_question)
        except Exception as e:
            print("âš ï¸ ì˜ì–´ ì§ˆë¬¸ í•œê¸€ ë²ˆì—­ ì‹¤íŒ¨:", e)
            print("âœ… Query Received:", query.model_dump())
            translated_question = query.question  # fallback

    law_text = fetch_combined_laws_cached((
        "ë¶€ê°€ê°€ì¹˜ì„¸ë²•", "ë¶€ê°€ê°€ì¹˜ì„¸ë²•ì‹œí–‰ë ¹", "ë¶€ê°€ê°€ì¹˜ì„¸ë²•ì‹œí–‰ê·œì¹™"
    ), lang=query.lang)

    prec_text = fetch_precedents_full(query.question, max_count=3)


    result = ask_with_law_context(
        translated_question,
        law_text,
        precedents=prec_text,
        lang=query.lang,
        original_question=clean_input(query.question)
    )

    if isinstance(result, dict):
        return result

    answer, refs, summaries, mappings = result
    refs = deduplicate_refs(refs)
    translated_names = {
        normalize_ref(ref): format_english_law_tag(smart_translate_law_tag(ref))
        for ref in refs
    }

    if query.lang == "en":
        sorted_refs = sorted(refs, key=lambda r: -len(r))
        for ref in sorted_refs:
            norm = normalize_ref(ref)
            eng_ref = translated_names.get(norm)
            if eng_ref and f"[{ref}]" in answer:
                answer = answer.replace(f"[{ref}]", f"[{eng_ref}]")
                summaries = {
                    k: v.replace(f"[{ref}]", f"[{eng_ref}]") if f"[{ref}]" in v else v
                    for k, v in summaries.items()
                }
                mappings = {
                    k: v.replace(f"[{ref}]", f"[{eng_ref}]") if f"[{ref}]" in v else v
                    for k, v in mappings.items()
                }

        ai_parts = extract_all_law_parts_gpt(answer)
        for part in ai_parts:
            norm = normalize_ref(part)
            eng = smart_translate_law_tag(part)
            # âœ… ì¤‘ë³µ ì¹˜í™˜ ë°©ì§€ ì¶”ê°€
            if eng and part in answer:
                answer = answer.replace(part, eng)
                summaries = {
                    k: v.replace(part, eng) if part in v else v
                    for k, v in summaries.items()
                }
                mappings = {
                    k: v.replace(part, eng) if part in v else v
                    for k, v in mappings.items()
                }

    confidence = self_rate_answer(query.question, answer)
    _, omission_stats = extract_law_references_and_omission(answer)

    answer = fix_tag_spacing(answer)
    summaries = {k: fix_tag_spacing(v) for k, v in summaries.items()}
    mappings = {k: fix_tag_spacing(v) for k, v in mappings.items()}
    summary = extract_summary(answer, query.lang)
    # ğŸ” ì§ˆë¬¸ ì‘ë‹µ ì²˜ë¦¬ ëë‚˜ê³  ìë™ ë¦¬í¬íŠ¸ ìƒì„±
    gpt_cost_report()

    return {
        "answer": answer,
        "references": refs,
        "summaries": summaries,
        "mappings": mappings,
        "confidence": confidence,
        "omission_stats": omission_stats,
        "translated_names": translated_names,
        "law_text": law_text,
        "summary": summary 
    }

def gpt_cost_report(log_path="logs/gpt_calls.csv", save_path="static/report.png"):
    try:
        df = pd.read_csv(log_path, on_bad_lines='skip')
    except Exception as e:
        print(f"âš ï¸ ë¡œê·¸ ë¡œë”© ì‹¤íŒ¨: {e}")
        return

    if df.empty:
        print("âš ï¸ ë¡œê·¸ ë°ì´í„° ì—†ìŒ")
        return

    model_costs = df.groupby("model")["cost"].sum()
    model_duration = df.groupby("model")["duration"].mean() if "duration" in df.columns else None
    purpose_counts = df["purpose"].value_counts() if "purpose" in df.columns else None

    fig, axes = plt.subplots(1, 3, figsize=(18, 5))

    model_costs.plot(kind="bar", ax=axes[0], color="skyblue")
    axes[0].set_title("ğŸ’° ëª¨ë¸ë³„ ì´ ë¹„ìš©")
    axes[0].set_ylabel("USD ($)")
    axes[0].set_xlabel("Model")

    if purpose_counts is not None:
        purpose_counts.plot(kind="bar", ax=axes[1], color="salmon")
        axes[1].set_title("ğŸ“¦ GPT ê¸°ëŠ¥ë³„ ì‚¬ìš© ë¶„í¬")
        axes[1].set_ylabel("í˜¸ì¶œ íšŸìˆ˜")
        axes[1].set_xlabel("Purpose")

    if model_duration is not None:
        model_duration.plot(kind="bar", ax=axes[2], color="lightgreen")
        axes[2].set_title("â± ëª¨ë¸ë³„ í‰ê·  ì‘ë‹µ ì‹œê°„")
        axes[2].set_ylabel("Seconds")
        axes[2].set_xlabel("Model")

    plt.tight_layout()
    try:
        plt.savefig(save_path)
        print(f"ğŸ“Š GPT ë¦¬í¬íŠ¸ ì €ì¥ ì™„ë£Œ â†’ {save_path}")
    except Exception as e:
        print(f"âŒ ì´ë¯¸ì§€ ì €ì¥ ì‹¤íŒ¨: {e}")
    finally:
        plt.close()


@app.get("/")
async def root():
    return FileResponse("static/index.html")
